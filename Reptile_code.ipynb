{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reptile code",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKWMmcrMLSD7"
      },
      "source": [
        "# Few-Shot learning with Reptile\n",
        "\n",
        "**Author:** [ADMoreau](https://github.com/ADMoreau)<br>\n",
        "**Date created:** 2020/05/21<br>\n",
        "**Last modified:** 2020/05/30<br>\n",
        "**Description:** Few-shot classification of the Omniglot dataset using Reptile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym8w9VufLSD9"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The [Reptile](https://arxiv.org/abs/1803.02999) algorithm was developed by OpenAI to\n",
        "perform model agnostic meta-learning. Specifically, this algorithm was designed to\n",
        "quickly learn to perform new tasks with minimal training (few-shot learning).\n",
        "The algorithm works by performing Stochastic Gradient Descent using the\n",
        "difference between weights trained on a mini-batch of never before seen data and the\n",
        "model weights prior to training over a fixed number of meta-iterations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWIXQobCtzET"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr3gSVeCt-kt"
      },
      "source": [
        "Code Sources \n",
        "\n",
        "Few-shot learning with reptile: https://keras.io/examples/vision/rept...\n",
        "On First-Order Meta Learning: https://arxiv.org/pdf/1803.02999.pdf\n",
        "MAML: https://arxiv.org/pdf/1703.03400.pdf\n",
        "Generative Teaching Networks: https://arxiv.org/pdf/1912.07768.pdf\n",
        "Teaching with Commentaries: https://arxiv.org/pdf/2011.03037.pdf\n",
        "Meta Pseudo Labels: https://arxiv.org/pdf/2003.10580.pdf\n",
        "\n",
        "*El código también mantiene sus comentarios originales en ingles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6-c5YDXLSD-"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rv_5a8yLSD-"
      },
      "source": [
        "## Define the Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD770ASXLSD-"
      },
      "source": [
        "\n",
        "# These hyperparameters define de numer of meta iterations\n",
        "\n",
        "learning_rate = 0.003\n",
        "meta_step_size = 0.25\n",
        "\n",
        "inner_batch_size = 25\n",
        "eval_batch_size = 25\n",
        "\n",
        "meta_iters = 2000\n",
        "eval_iters = 5\n",
        "\n",
        "# Steps inside sampled tasks  \n",
        "inner_iters = 4\n",
        "\n",
        "\n",
        "eval_interval = 1\n",
        "# Hyperparameter for the number of inner sampling \n",
        "train_shots = 20\n",
        "shots = 5\n",
        "classes = 5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6h2hWu5LSD_"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "The [Omniglot dataset](https://github.com/brendenlake/omniglot/) is a dataset of 1,623\n",
        "characters taken from 50 different alphabets, with 20 examples for each character.\n",
        "The 20 samples for each character were drawn online via Amazon's Mechanical Turk. For the\n",
        "few-shot learning task, `k` samples (or \"shots\") are drawn randomly from `n` randomly-chosen\n",
        "classes. These `n` numerical values are used to create a new set of temporary labels to use\n",
        "to test the model's ability to learn a new task given few examples. In other words, if you\n",
        "are training on 5 classes, your new class labels will be either 0, 1, 2, 3, or 4.\n",
        "Omniglot is a great dataset for this task since there are many different classes to draw\n",
        "from, with a reasonable number of samples for each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoknurqzLSD_"
      },
      "source": [
        "\n",
        "# Define our data loader. So we have a class data set that´s going to be sampling tasks from the omniglot data set \n",
        "class Dataset:\n",
        "# This class will facilitate the creation of a few-shot dataset\n",
        "    # from the Omniglot dataset that can be sampled from quickly while also\n",
        "    # allowing to create new labels at the same time.\n",
        "\n",
        "\n",
        "# We have our initialization\n",
        "    def __init__(self, training):\n",
        "        # We set the splits\n",
        "        split = \"train\" if training else \"test\"\n",
        "        # We load the omniglot dataset from our tensorflow datasets library\n",
        "        ds = tfds.load(\"omniglot\", split=split, as_supervised=True, shuffle_files=False)\n",
        "        # Iterate over the dataset to get each individual image and its class,\n",
        "        # and put that data into a dictionary.\n",
        "        self.data = {}\n",
        "\n",
        "# Extraction, where we do the convert the image to the float32 data points\n",
        "        def extraction(image, label):\n",
        "            # This function will shrink the Omniglot images to the desired size,\n",
        "            # scale pixel values and convert the RGB image to grayscale\n",
        "            image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "            # Move it into grayscale\n",
        "            image = tf.image.rgb_to_grayscale(image)\n",
        "\n",
        "            # Resize\n",
        "            image = tf.image.resize(image, [28, 28])\n",
        "            return image, label\n",
        "\n",
        "# We use ds.map this extraction function. This is how we loop through the data and then map it by applying \n",
        "# this function to each instance in the data set\n",
        "        for image, label in ds.map(extraction):\n",
        "\n",
        "          # We are going to convert our image to a numpy array\n",
        "            image = image.numpy()\n",
        "          # The label to a string in the numpy array because we are not using the original label from omniglot\n",
        "            label = str(label.numpy())\n",
        "\n",
        "            # we are using this to index the data set\n",
        "            if label not in self.data:\n",
        "                self.data[label] = []\n",
        "            self.data[label].append(image)\n",
        "        self.labels = list(self.data.keys())\n",
        "\n",
        "\n",
        "# Now we are in the meat of our dataset loading class. \n",
        "\n",
        "# Sampling this new mini data set as we sample a random sets of the characters and then assign these new labels\n",
        "# based on the arbitrary configuration of these new classes in a new classification problem\n",
        "    def get_mini_dataset(\n",
        "        self, batch_size, repetitions, shots, num_classes, split=False\n",
        "    ):\n",
        "\n",
        "# We assign this placeholder numpy array for the labels and for the images\n",
        "        temp_labels = np.zeros(shape=(num_classes * shots))\n",
        "        temp_images = np.zeros(shape=(num_classes * shots, 28, 28, 1))\n",
        "        if split:\n",
        "            test_labels = np.zeros(shape=(num_classes))\n",
        "            test_images = np.zeros(shape=(num_classes, 28, 28, 1))\n",
        "\n",
        "\n",
        "\n",
        "        # Get a random subset of labels from the entire label set.\n",
        "        label_subset = random.choices(self.labels, k=num_classes)\n",
        "# We loop through the class index class object in enumerate label subsets so this is our label subset we are looping\n",
        "# through the five different randomly selected labels\n",
        "        for class_idx, class_obj in enumerate(label_subset):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Use enumerated index value as a temporary label for mini-batch in\n",
        "            # few shot learning.\n",
        "            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n",
        "            # If creating a split dataset for testing, select an extra sample from each\n",
        "            # label to create the test dataset.\n",
        "\n",
        "\n",
        "\n",
        "# This is how we overwrite the data with out selection of the new images for our selected class index \n",
        "\n",
        "            if split:\n",
        "                test_labels[class_idx] = class_idx\n",
        "                images_to_split = random.choices(\n",
        "                    self.data[label_subset[class_idx]], k=shots + 1\n",
        "                )\n",
        "                test_images[class_idx] = images_to_split[-1]\n",
        "                temp_images[\n",
        "                    class_idx * shots : (class_idx + 1) * shots\n",
        "                ] = images_to_split[:-1]\n",
        "# If we dont do the split, then we just overwrite our temporary images by just indexing into the original set with our\n",
        "# index of the label subset as we are looping through the class index \n",
        "            else:\n",
        "                # For each index in the randomly selected label_subset, sample the\n",
        "                # necessary number of images.\n",
        "                temp_images[\n",
        "                    class_idx * shots : (class_idx + 1) * shots\n",
        "                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n",
        "\n",
        "\n",
        "\n",
        "# Overall this is going to return these nested data sets that we are going to be using to fit these classification \n",
        "# problems and update our global parameters by looking at the direction each of these new subtasks pull it towards\n",
        "\n",
        "# We take this new nested data set and we cast our labels to integers for assigning the zero, one, two, three and\n",
        "# four labels\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n",
        "        )\n",
        "        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)\n",
        "        if split:\n",
        "            return dataset, test_images, test_labels\n",
        "        return dataset\n",
        "\n",
        "\n",
        "import urllib3\n",
        "\n",
        "urllib3.disable_warnings()  # Disable SSL warnings that may happen during download.\n",
        "train_dataset = Dataset(training=True)\n",
        "test_dataset = Dataset(training=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYwZtJWuLSEB"
      },
      "source": [
        "## Visualize some examples from the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKARDQcdLSEB"
      },
      "source": [
        "# Visualizing some images from the omniglot data set\n",
        "\n",
        "# Define a 5x5 matrix of visualizing these images\n",
        "_, axarr = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))\n",
        "\n",
        "# We overwrite each position and sampled the next set from the trained data set loader\n",
        "sample_keys = list(train_dataset.data.keys())\n",
        "\n",
        "for a in range(5):\n",
        "    for b in range(5):\n",
        "        temp_image = train_dataset.data[sample_keys[a]][b]\n",
        "        temp_image = np.stack((temp_image[:, :, 0],) * 3, axis=2)\n",
        "        temp_image *= 255\n",
        "        temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")\n",
        "        if b == 2:\n",
        "            axarr[a, b].set_title(\"Class : \" + sample_keys[a])\n",
        "        axarr[a, b].imshow(temp_image, cmap=\"gray\")\n",
        "        axarr[a, b].xaxis.set_visible(False)\n",
        "        axarr[a, b].yaxis.set_visible(False)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj5gygSkLSEB"
      },
      "source": [
        "## Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYmx7oGNLSEC"
      },
      "source": [
        "# We are stacking these convolution batch norm ReLU blocks and we do this four times and then \n",
        "def conv_bn(x):\n",
        "    x = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return layers.ReLU()(x)\n",
        "\n",
        "\n",
        "inputs = layers.Input(shape=(28, 28, 1))\n",
        "x = conv_bn(inputs)\n",
        "x = conv_bn(x)\n",
        "x = conv_bn(x)\n",
        "x = conv_bn(x)\n",
        "\n",
        "# we flatten it into a vector\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "# connect it to our five class classification problem\n",
        "outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile()\n",
        "optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0ySD9ALSEC"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nQ9hfgNLSEC"
      },
      "source": [
        "# We define these two arrays to store the losses \n",
        "training = []\n",
        "testing = []\n",
        "\n",
        "# We are looping through the number of meta iterations\n",
        "for meta_iter in range(meta_iters):\n",
        "    frac_done = meta_iter / meta_iters\n",
        "    # We derived this meta step size \n",
        "    cur_meta_step_size = (1 - frac_done) * meta_step_size\n",
        "    # Temporarily save the weights from the model.\n",
        "    old_vars = model.get_weights()\n",
        "    # Get a sample from the full dataset.\n",
        "    # One mini data set per update\n",
        "    mini_dataset = train_dataset.get_mini_dataset(\n",
        "        inner_batch_size, inner_iters, train_shots, classes\n",
        "    )\n",
        "\n",
        "    for images, labels in mini_dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = model(images)\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    new_vars = model.get_weights()\n",
        "    # Perform SGD for the meta step.\n",
        "# When we are doing this global update as we are updating our global parameters and direction of the new parameters\n",
        "# we are going to have this update rule \n",
        "\n",
        "# gradient descent where its the old parameters plus the gradient times the learning rate\n",
        "    for var in range(len(new_vars)):\n",
        "        new_vars[var] = old_vars[var] + (\n",
        "            (new_vars[var] - old_vars[var]) * cur_meta_step_size\n",
        "        )\n",
        "\n",
        "    # Once we have finished our inner update we are now going to the evaluation step\n",
        "\n",
        "    # After the meta-learning step, reload the newly-trained weights into the model.\n",
        "    model.set_weights(new_vars)\n",
        "    # Evaluation loop\n",
        "    if meta_iter % eval_interval == 0:\n",
        "        accuracies = []\n",
        "        for dataset in (train_dataset, test_dataset):\n",
        "            # Sample a mini dataset from the full dataset.\n",
        "            train_set, test_images, test_labels = dataset.get_mini_dataset(\n",
        "                eval_batch_size, eval_iters, shots, classes, split=True\n",
        "            )\n",
        "            old_vars = model.get_weights()\n",
        "            # Train on the samples and get the resulting accuracies.\n",
        "            for images, labels in train_set:\n",
        "                with tf.GradientTape() as tape:\n",
        "                    preds = model(images)\n",
        "                    loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\n",
        "                grads = tape.gradient(loss, model.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "# Task predictions\n",
        "            test_preds = model.predict(test_images)\n",
        "            test_preds = tf.argmax(test_preds).numpy()\n",
        "            num_correct = (test_preds == test_labels).sum()\n",
        "            # Reset the weights after getting the evaluation accuracies.\n",
        "            model.set_weights(old_vars)\n",
        "            accuracies.append(num_correct / classes)\n",
        "        training.append(accuracies[0])\n",
        "        testing.append(accuracies[1])\n",
        "        if meta_iter % 100 == 0:\n",
        "            print(\n",
        "                \"batch %d: train=%f test=%f\" % (meta_iter, accuracies[0], accuracies[1])\n",
        "            )\n",
        "\n",
        "            # That´s how we define our training loop updating the global inialization parameters and the how we\n",
        "            # have a evaluation loop that does not actually update the parameters.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS6PnQ7TLSED"
      },
      "source": [
        "## Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leBXz9VvLSEE"
      },
      "source": [
        "# First, some preprocessing to smooth the training and testing arrays for display.\n",
        "window_length = 100\n",
        "train_s = np.r_[\n",
        "    training[window_length - 1 : 0 : -1], training, training[-1:-window_length:-1]\n",
        "]\n",
        "test_s = np.r_[\n",
        "    testing[window_length - 1 : 0 : -1], testing, testing[-1:-window_length:-1]\n",
        "]\n",
        "w = np.hamming(window_length)\n",
        "train_y = np.convolve(w / w.sum(), train_s, mode=\"valid\")\n",
        "test_y = np.convolve(w / w.sum(), test_s, mode=\"valid\")\n",
        "\n",
        "# Display the training accuracies.\n",
        "x = np.arange(0, len(test_y), 1)\n",
        "plt.plot(x, test_y, x, train_y)\n",
        "plt.legend([\"test\", \"train\"])\n",
        "plt.grid()\n",
        "\n",
        "train_set, test_images, test_labels = dataset.get_mini_dataset(\n",
        "    eval_batch_size, eval_iters, shots, classes, split=True\n",
        ")\n",
        "for images, labels in train_set:\n",
        "    with tf.GradientTape() as tape:\n",
        "        preds = model(images)\n",
        "        loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\n",
        "    grads = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "test_preds = model.predict(test_images)\n",
        "test_preds = tf.argmax(test_preds).numpy()\n",
        "\n",
        "_, axarr = plt.subplots(nrows=1, ncols=5, figsize=(20, 20))\n",
        "\n",
        "sample_keys = list(train_dataset.data.keys())\n",
        "\n",
        "for i, ax in zip(range(5), axarr):\n",
        "    temp_image = np.stack((test_images[i, :, :, 0],) * 3, axis=2)\n",
        "    temp_image *= 255\n",
        "    temp_image = np.clip(temp_image, 0, 255).astype(\"uint8\")\n",
        "    ax.set_title(\n",
        "        \"Label : {}, Prediction : {}\".format(int(test_labels[i]), test_preds[i])\n",
        "    )\n",
        "    ax.imshow(temp_image, cmap=\"gray\")\n",
        "    ax.xaxis.set_visible(False)\n",
        "    ax.yaxis.set_visible(False)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}